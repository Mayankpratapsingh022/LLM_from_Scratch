{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iph6bdR7OSC6"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"> Reading in a short story as text sample into Python. </div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SW3wRdqZOoTz"
      },
      "outputs": [],
      "source": [
        "with open(\"text_data/the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Creating Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w_1BNrtO3Vl"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "The print command prints the total number of characters followed by the first 100 characters of this file for illustration purposes.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFVcQYrhVKZO",
        "outputId": "687767d2-4c5c-4ff3-f769-4039dfd33b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total number of charater :  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "print(\"total number of charater : \", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">So the goal is to tokenize this 20,479 character short story into individual words and special charaters  taht we an then turn inot embeddings for LLM training </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mWwXHdMW611"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"> Using some simple example text, we can use the re.split command with the following syntax to split a text on whitespace </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OotkYN2GV2B6",
        "outputId": "677b7358-9d67-4c4b-86f2-4ad7749e23f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)',text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60lFqRr1l7QO"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">The result is a list of individual words , whitespaces, and punctuation charaters </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQSr25Q-XL6X",
        "outputId": "68f36592-d9d6-4ffe-f76c-35c527493c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ],
      "source": [
        "# let's modify the regular expression splits on whitespaces (\\s) and commans, and full stop\n",
        "result = re.split(r'([,.]|\\s)',text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUrKQ3twmbf1"
      },
      "source": [
        "we can see that the words and punctuation charaters are now separate list entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-danger\"> A small remaining  issue is that the list still includes whitespace characters. Optionally, we can remove these redundant characters safely as follows </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCrA9Ahum022",
        "outputId": "b561e894-9b14-44db-97e9-9ca38bce5970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvLjfWTvnDJH"
      },
      "source": [
        "#### Removing whitespaces or not\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J89qgKsm87-",
        "outputId": "498d4101-0c96-4ac2-e867-c1e8cdb639ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FECBD1lxJcG",
        "outputId": "dc52daf6-4ceb-4d80-952e-0ce71155990d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "# Strip whitespace from each item and then filter out any empty strings.\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">Testing with a sample text </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q_zTo_95xzmI"
      },
      "outputs": [],
      "source": [
        "text = \"Hello, world. Is this-- a test?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIgMHhS9x3tS",
        "outputId": "f6306973-806a-4fa6-bd5a-e10b6d9701dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWrM7i2NyRJm"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> Now that we got a basic tokenizer working, let's apply it to Edith Wharton's entire short story: </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXoyJc8UyGAf",
        "outputId": "3f46fa1d-e255-4098-835a-699ee0937b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius--though', 'a', 'good', 'fellow', 'enough--so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his']\n"
          ]
        }
      ],
      "source": [
        "preprocessed = re.split(r'([,.:;?_\\'\"()\\[\\]|\\s])', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekKxBi2pycAk",
        "outputId": "988ba2b9-97bc-446a-e15b-22d15441b236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4481\n"
          ]
        }
      ],
      "source": [
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr14kXDKz3BB"
      },
      "source": [
        "# Step 2 : Creating Token IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfXz8O-az6lV"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "In the previous section, we tokenized Edith Wharton's short story and assigned it to a Python variable called preprocessed.\n",
        "\n",
        "Let's create a list of unique tokens and sort them alphabetically to determine the vocabulary size </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgT2IH0_zUzC",
        "outputId": "b9e63bcb-ea6e-4e3e-a483-039e147c2333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1217\n"
          ]
        }
      ],
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4s_kmzs1jeG"
      },
      "source": [
        "After determining that the vocabulary size is 1,217 via the above code, we create the vocabulary and print its 51 entries for illustration purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mxNuPygw00m-"
      },
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer, token in enumerate(all_words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_JD2po31_zU",
        "outputId": "3c38de84-22ae-47e0-e0ca-7691df743c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('\"', 0)\n",
            "(\"'\", 1)\n",
            "('(', 2)\n",
            "(')', 3)\n",
            "(',', 4)\n",
            "('--and', 5)\n",
            "('--it', 6)\n",
            "('--oh', 7)\n",
            "('--she', 8)\n",
            "('--that', 9)\n",
            "('.', 10)\n",
            "(':', 11)\n",
            "(';', 12)\n",
            "('?', 13)\n",
            "('A', 14)\n",
            "('Ah', 15)\n",
            "('Ah--I', 16)\n",
            "('Among', 17)\n",
            "('And', 18)\n",
            "('Are', 19)\n",
            "('Arrt', 20)\n",
            "('As', 21)\n",
            "('At', 22)\n",
            "('Be', 23)\n",
            "('Begin', 24)\n",
            "('Burlington', 25)\n",
            "('But', 26)\n",
            "('By', 27)\n",
            "('Carlo', 28)\n",
            "('Chicago', 29)\n",
            "('Claude', 30)\n",
            "('Come', 31)\n",
            "('Croft', 32)\n",
            "('Destroyed', 33)\n",
            "('Devonshire', 34)\n",
            "('Don', 35)\n",
            "('Dubarry', 36)\n",
            "('Emperors', 37)\n",
            "('Florence', 38)\n",
            "('For', 39)\n",
            "('Gallery', 40)\n",
            "('Gideon', 41)\n",
            "('Gisburn', 42)\n",
            "('Gisburn!', 43)\n",
            "('Gisburn--as', 44)\n",
            "('Gisburn--fond', 45)\n",
            "('Gisburns', 46)\n",
            "('Grafton', 47)\n",
            "('Greek', 48)\n",
            "('Grindle', 49)\n",
            "('Grindles', 50)\n"
          ]
        }
      ],
      "source": [
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >=50:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB6Qa6LV2aew"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> As we can see, based on the output above, the dictionary contains individual tokens associated with unique integer labels. </div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUU85rtm2uj6"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> Later, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs back into text.\n",
        "\n",
        "For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens. </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV_rryFm3OwB"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> Let's implement a complete tokenizer class in Python.\n",
        "\n",
        "The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce the tokens\n",
        "\n",
        "In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token Id back to the string </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y9VGO1LN2G6F"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # Replace spaces before the specified punctuations\n",
        "\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQDoY5t77r3v"
      },
      "source": [
        "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenizer a passage from Edith Wharton short story to try it out in practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3zs-Ok6W5bfu"
      },
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\n",
        "\"It's the last he painted,you know,\"\n",
        "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDRjcDmg8K70",
        "outputId": "e2e103ff-9d80-4d56-caa2-c86bf1903f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 63, 1, 920, 1065, 646, 563, 808, 4, 1212, 640, 4, 0, 78, 10, 42, 921, 1191, 818, 863, 10]\n"
          ]
        }
      ],
      "source": [
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4GsuVMp88aH"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> The code above prints the following token IDs: Next, let's see if we can turn these token IDs back into text using the decode method: </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NpoKSISa8mWC",
        "outputId": "08dfe470-aa7a-43b7-b298-5420fb305686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho_YEMyZ-6Wi"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> Based on the output above, we can see that the decode method successfully converted the token IDs back into the original text. </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUis5ld__DnA"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing text based on a snippet from the training set: </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "aS4e4h8O9Rig",
        "outputId": "9e3b099f-d57e-4f53-9548-304a029966b2"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'Hello'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like phone?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
            "Cell \u001b[1;32mIn[15], line 13\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m,text)\n\u001b[0;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     12\u001b[0m ]\n\u001b[1;32m---> 13\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
            "Cell \u001b[1;32mIn[15], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m,text)\n\u001b[0;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     12\u001b[0m ]\n\u001b[1;32m---> 13\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ],
      "source": [
        "text = \"Hello, do you like phone?\"\n",
        "print(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw8PFMI3_jMV"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\"> The problem is that the world <b> \"Hello\" </b> was not used in the <b>The Verdict </b> short story.\n",
        "\n",
        "Hence, it is not contained in the vocabulary.\n",
        "\n",
        "This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs. </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq_HKeRs_4JC"
      },
      "source": [
        "## ADDING SPECIAL CONTEXT TOKENS\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "In the previous section, I have Implemented a simple tokenizer and applied it to a passage from the training set.\n",
        "\n",
        "In this section, we will modify this tokenizer to handle unknown words.\n",
        "\n",
        "In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, <b><|unk|></b> and <b><|endoftext|></b> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Lvam3jA9Id"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "we can modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
        "\n",
        "Furthermore, we add a token between unrealated texts.\n",
        "\n",
        "For example, when training GPT-like LLMs on multiple independent documents or books , it is common to insert before each document or book that follows a previous text source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGJclP0NDX3n"
      },
      "source": [
        "#### Let's now modify the vocabulary to include these two special tokens, and <|endoftext|>, by adding the previous section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dmknSuS5_bE3"
      },
      "outputs": [],
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endiftext|>\",\"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhKQBLg-D8QP",
        "outputId": "d4c8e60a-611b-4626-aab9-241ef4cb7a6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1219"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_5TvDnKEox8"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> Based on the output of the print statement above, the new vocabulary size is 1219 </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b235ZIAEulh"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\"> As an additional quick check, let's print the last 5 entries of the updated vocabulary </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG2pGaYCEMjv",
        "outputId": "fac0d0de-742a-488d-f805-41f747bb46c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('younger', 1214)\n",
            "('your', 1215)\n",
            "('yourself', 1216)\n",
            "('<|endiftext|>', 1217)\n",
            "('<|unk|>', 1218)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm-5854zFDWV"
      },
      "source": [
        "#### A simple text tokenizer than handles unknown words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qll8VMdFMgQ"
      },
      "source": [
        "Step 1: Replace unknown words by <|unk|> tokens\n",
        "\n",
        "Step 2: Replace spaces before the specified punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "L2ao_UOCFA69"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_\\'\"()\\[\\]|\\s])', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "    def decode(self,ids):\n",
        "      text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "      # Replace spaces before the specified punctuations\n",
        "\n",
        "      text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "      return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8r5ffC0NGVW8"
      },
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \"<|endoftext|>\".join((text1,text2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBOQxCX-GsUT",
        "outputId": "14e4876c-d9d3-41c5-cc0e-4a055021ca8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-biquzuGtEk",
        "outputId": "28e6ad3d-f490-4daa-a0ec-539068fe45da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1218,\n",
              " 4,\n",
              " 381,\n",
              " 1212,\n",
              " 673,\n",
              " 1052,\n",
              " 13,\n",
              " 1218,\n",
              " 1218,\n",
              " 1218,\n",
              " 1218,\n",
              " 1218,\n",
              " 1065,\n",
              " 1031,\n",
              " 1061,\n",
              " 778,\n",
              " 1065,\n",
              " 1218,\n",
              " 10]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YWFwoNOmGx9z",
        "outputId": "4db99be4-c2d3-454a-b7e7-2223770a2723"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> the sunlit terraces of the <|unk|>.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7KPZvtEJ6Xw"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">Based on comparing the de-tokenized text above with the original input text, we know that the training dataset, Edith Wharton's short story The Verdict, did not contain the words <b> \"Hello\"</b> and <b> \"palace\"</b> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J88XKzM-KQtt"
      },
      "source": [
        "So far, we have discussed tokenization as an essential step in processing text as input to LLMs. Depending on the LLM, some researchers also consider additional special tokens such as the following:\n",
        "\n",
        "- **[BOS] (beginning of sequence):** This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
        "\n",
        "- **[EOS] (end of sequence):** This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to `<endoftext>`. For instance, when combining two different Wikipedia articles or books, the `[EOS]` token indicates where one article ends and the next one begins.\n",
        "\n",
        "- **[PAD] (padding):** When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the `[PAD]` token, up to the length of the longest text in the batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thF25l-oLGMc"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"> Note that the tokenizer used for GPT models does not need any of these tokens mentioned above but only uses an <b> <|endoftext|> </b> token for simplicity </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lh0nacrLlWF"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"> The tokenizer used for GPT models also doesn't use an <|unk|> token for out ofvocabulary words. Instead, GPT model use a <b> byte pair encoding tokenizer </b>, which breaks down words into subword units </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtbqm78IL73W"
      },
      "source": [
        "# BYTE PAIR ENCODING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "We have implemented a <b> Simple Tokenization </b> scheme in the previous sections for illustraton purpose.\n",
        "\n",
        "This section covers a more sophisticated tokenization scheme based on a concept called <b>byte pair encoding (BPE).</b>\n",
        "\n",
        "The BPE tokenizer covered in this section was used to train LLMs such as GPT-2,GPT-3, and the original model used in ChatGPT\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "So The implementing BPE can be relatively complicated, we will use an existing Python open-source library called tiktoken (openai uses it )\n",
        "\n",
        "This library implements the BPE algorithm very efficiently basaed on source code in Rust.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P67h2F-WJzkh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llmstrach",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
