{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iph6bdR7OSC6"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"> Reading in a short story as text sample into Python. </div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SW3wRdqZOoTz"
      },
      "outputs": [],
      "source": [
        "with open(\"text_data/the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Creating Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w_1BNrtO3Vl"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "The print command prints the total number of characters followed by the first 100 characters of this file for illustration purposes.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFVcQYrhVKZO",
        "outputId": "687767d2-4c5c-4ff3-f769-4039dfd33b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total number of charater :  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "print(\"total number of charater : \", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">So the goal is to tokenize this 20,479 character short story into individual words and special charaters  taht we an then turn inot embeddings for LLM training </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mWwXHdMW611"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"> Using some simple example text, we can use the re.split command with the following syntax to split a text on whitespace </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OotkYN2GV2B6",
        "outputId": "677b7358-9d67-4c4b-86f2-4ad7749e23f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)',text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60lFqRr1l7QO"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">The result is a list of individual words , whitespaces, and punctuation charaters </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQSr25Q-XL6X",
        "outputId": "68f36592-d9d6-4ffe-f76c-35c527493c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ],
      "source": [
        "# let's modify the regular expression splits on whitespaces (\\s) and commans, and full stop\n",
        "result = re.split(r'([,.]|\\s)',text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUrKQ3twmbf1"
      },
      "source": [
        "we can see that the words and punctuation charaters are now separate list entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-danger\"> A small remaining  issue is that the list still includes whitespace characters. Optionally, we can remove these redundant characters safely as follows </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCrA9Ahum022",
        "outputId": "b561e894-9b14-44db-97e9-9ca38bce5970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvLjfWTvnDJH"
      },
      "source": [
        "#### Removing whitespaces or not\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J89qgKsm87-",
        "outputId": "498d4101-0c96-4ac2-e867-c1e8cdb639ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FECBD1lxJcG",
        "outputId": "dc52daf6-4ceb-4d80-952e-0ce71155990d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "# Strip whitespace from each item and then filter out any empty strings.\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">Testing with a sample text </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q_zTo_95xzmI"
      },
      "outputs": [],
      "source": [
        "text = \"Hello, world. Is this-- a test?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIgMHhS9x3tS",
        "outputId": "f6306973-806a-4fa6-bd5a-e10b6d9701dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWrM7i2NyRJm"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> Now that we got a basic tokenizer working, let's apply it to Edith Wharton's entire short story: </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXoyJc8UyGAf",
        "outputId": "3f46fa1d-e255-4098-835a-699ee0937b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius--though', 'a', 'good', 'fellow', 'enough--so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his']\n"
          ]
        }
      ],
      "source": [
        "preprocessed = re.split(r'([,.:;?_\\'\"()\\[\\]|\\s])', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekKxBi2pycAk",
        "outputId": "988ba2b9-97bc-446a-e15b-22d15441b236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4481\n"
          ]
        }
      ],
      "source": [
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr14kXDKz3BB"
      },
      "source": [
        "# Step 2 : Creating Token IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfXz8O-az6lV"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "In the previous section, we tokenized Edith Wharton's short story and assigned it to a Python variable called preprocessed.\n",
        "\n",
        "Let's create a list of unique tokens and sort them alphabetically to determine the vocabulary size </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgT2IH0_zUzC",
        "outputId": "b9e63bcb-ea6e-4e3e-a483-039e147c2333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1217\n"
          ]
        }
      ],
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4s_kmzs1jeG"
      },
      "source": [
        "After determining that the vocabulary size is 1,217 via the above code, we create the vocabulary and print its 51 entries for illustration purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mxNuPygw00m-"
      },
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer, token in enumerate(all_words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_JD2po31_zU",
        "outputId": "3c38de84-22ae-47e0-e0ca-7691df743c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('\"', 0)\n",
            "(\"'\", 1)\n",
            "('(', 2)\n",
            "(')', 3)\n",
            "(',', 4)\n",
            "('--and', 5)\n",
            "('--it', 6)\n",
            "('--oh', 7)\n",
            "('--she', 8)\n",
            "('--that', 9)\n",
            "('.', 10)\n",
            "(':', 11)\n",
            "(';', 12)\n",
            "('?', 13)\n",
            "('A', 14)\n",
            "('Ah', 15)\n",
            "('Ah--I', 16)\n",
            "('Among', 17)\n",
            "('And', 18)\n",
            "('Are', 19)\n",
            "('Arrt', 20)\n",
            "('As', 21)\n",
            "('At', 22)\n",
            "('Be', 23)\n",
            "('Begin', 24)\n",
            "('Burlington', 25)\n",
            "('But', 26)\n",
            "('By', 27)\n",
            "('Carlo', 28)\n",
            "('Chicago', 29)\n",
            "('Claude', 30)\n",
            "('Come', 31)\n",
            "('Croft', 32)\n",
            "('Destroyed', 33)\n",
            "('Devonshire', 34)\n",
            "('Don', 35)\n",
            "('Dubarry', 36)\n",
            "('Emperors', 37)\n",
            "('Florence', 38)\n",
            "('For', 39)\n",
            "('Gallery', 40)\n",
            "('Gideon', 41)\n",
            "('Gisburn', 42)\n",
            "('Gisburn!', 43)\n",
            "('Gisburn--as', 44)\n",
            "('Gisburn--fond', 45)\n",
            "('Gisburns', 46)\n",
            "('Grafton', 47)\n",
            "('Greek', 48)\n",
            "('Grindle', 49)\n",
            "('Grindles', 50)\n"
          ]
        }
      ],
      "source": [
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >=50:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB6Qa6LV2aew"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> As we can see, based on the output above, the dictionary contains individual tokens associated with unique integer labels. </div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUU85rtm2uj6"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> Later, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs back into text.\n",
        "\n",
        "For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens. </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV_rryFm3OwB"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> Let's implement a complete tokenizer class in Python.\n",
        "\n",
        "The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce the tokens\n",
        "\n",
        "In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token Id back to the string </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y9VGO1LN2G6F"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self,vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "\n",
        "  def encode(self,text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # Replace spaces before the specified punctuations\n",
        "\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQDoY5t77r3v"
      },
      "source": [
        "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenizer a passage from Edith Wharton short story to try it out in practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3zs-Ok6W5bfu"
      },
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\n",
        "\"It's the last he painted,you know,\"\n",
        "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDRjcDmg8K70",
        "outputId": "e2e103ff-9d80-4d56-caa2-c86bf1903f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 63, 1, 920, 1065, 646, 563, 808, 4, 1212, 640, 4, 0, 78, 10, 42, 921, 1191, 818, 863, 10]\n"
          ]
        }
      ],
      "source": [
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4GsuVMp88aH"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> The code above prints the following token IDs: Next, let's see if we can turn these token IDs back into text using the decode method: </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NpoKSISa8mWC",
        "outputId": "08dfe470-aa7a-43b7-b298-5420fb305686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho_YEMyZ-6Wi"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> Based on the output above, we can see that the decode method successfully converted the token IDs back into the original text. </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUis5ld__DnA"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing text based on a snippet from the training set: </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "aS4e4h8O9Rig",
        "outputId": "9e3b099f-d57e-4f53-9548-304a029966b2"
      },
      "outputs": [],
      "source": [
        "# text = \"Hello, do you like phone?\"\n",
        "# print(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw8PFMI3_jMV"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\"> The problem is that the world <b> \"Hello\" </b> was not used in the <b>The Verdict </b> short story.\n",
        "\n",
        "Hence, it is not contained in the vocabulary.\n",
        "\n",
        "This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs. </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq_HKeRs_4JC"
      },
      "source": [
        "## ADDING SPECIAL CONTEXT TOKENS\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "In the previous section, I have Implemented a simple tokenizer and applied it to a passage from the training set.\n",
        "\n",
        "In this section, we will modify this tokenizer to handle unknown words.\n",
        "\n",
        "In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, <b><|unk|></b> and <b><|endoftext|></b> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Lvam3jA9Id"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "we can modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
        "\n",
        "Furthermore, we add a token between unrealated texts.\n",
        "\n",
        "For example, when training GPT-like LLMs on multiple independent documents or books , it is common to insert before each document or book that follows a previous text source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGJclP0NDX3n"
      },
      "source": [
        "#### Let's now modify the vocabulary to include these two special tokens, and <|endoftext|>, by adding the previous section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dmknSuS5_bE3"
      },
      "outputs": [],
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endiftext|>\",\"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhKQBLg-D8QP",
        "outputId": "d4c8e60a-611b-4626-aab9-241ef4cb7a6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1219"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_5TvDnKEox8"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\"> Based on the output of the print statement above, the new vocabulary size is 1219 </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b235ZIAEulh"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\"> As an additional quick check, let's print the last 5 entries of the updated vocabulary </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG2pGaYCEMjv",
        "outputId": "fac0d0de-742a-488d-f805-41f747bb46c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('younger', 1214)\n",
            "('your', 1215)\n",
            "('yourself', 1216)\n",
            "('<|endiftext|>', 1217)\n",
            "('<|unk|>', 1218)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm-5854zFDWV"
      },
      "source": [
        "#### A simple text tokenizer than handles unknown words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qll8VMdFMgQ"
      },
      "source": [
        "Step 1: Replace unknown words by <|unk|> tokens\n",
        "\n",
        "Step 2: Replace spaces before the specified punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "L2ao_UOCFA69"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_\\'\"()\\[\\]|\\s])', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "    def decode(self,ids):\n",
        "      text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "      # Replace spaces before the specified punctuations\n",
        "\n",
        "      text = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "      return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8r5ffC0NGVW8"
      },
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \"<|endoftext|>\".join((text1,text2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBOQxCX-GsUT",
        "outputId": "14e4876c-d9d3-41c5-cc0e-4a055021ca8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-biquzuGtEk",
        "outputId": "28e6ad3d-f490-4daa-a0ec-539068fe45da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1218,\n",
              " 4,\n",
              " 381,\n",
              " 1212,\n",
              " 673,\n",
              " 1052,\n",
              " 13,\n",
              " 1218,\n",
              " 1218,\n",
              " 1218,\n",
              " 1218,\n",
              " 1218,\n",
              " 1065,\n",
              " 1031,\n",
              " 1061,\n",
              " 778,\n",
              " 1065,\n",
              " 1218,\n",
              " 10]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YWFwoNOmGx9z",
        "outputId": "4db99be4-c2d3-454a-b7e7-2223770a2723"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> the sunlit terraces of the <|unk|>.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7KPZvtEJ6Xw"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">Based on comparing the de-tokenized text above with the original input text, we know that the training dataset, Edith Wharton's short story The Verdict, did not contain the words <b> \"Hello\"</b> and <b> \"palace\"</b> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J88XKzM-KQtt"
      },
      "source": [
        "So far, we have discussed tokenization as an essential step in processing text as input to LLMs. Depending on the LLM, some researchers also consider additional special tokens such as the following:\n",
        "\n",
        "- **[BOS] (beginning of sequence):** This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
        "\n",
        "- **[EOS] (end of sequence):** This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to `<endoftext>`. For instance, when combining two different Wikipedia articles or books, the `[EOS]` token indicates where one article ends and the next one begins.\n",
        "\n",
        "- **[PAD] (padding):** When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the `[PAD]` token, up to the length of the longest text in the batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thF25l-oLGMc"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"> Note that the tokenizer used for GPT models does not need any of these tokens mentioned above but only uses an <b> <|endoftext|> </b> token for simplicity </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lh0nacrLlWF"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"> The tokenizer used for GPT models also doesn't use an <|unk|> token for out ofvocabulary words. Instead, GPT model use a <b> byte pair encoding tokenizer </b>, which breaks down words into subword units </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtbqm78IL73W"
      },
      "source": [
        "# BYTE PAIR ENCODING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "We have implemented a <b> Simple Tokenization </b> scheme in the previous sections for illustraton purpose.\n",
        "\n",
        "This section covers a more sophisticated tokenization scheme based on a concept called <b>byte pair encoding (BPE).</b>\n",
        "\n",
        "The BPE tokenizer covered in this section was used to train LLMs such as GPT-2,GPT-3, and the original model used in ChatGPT\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "So The implementing BPE can be relatively complicated, we will use an existing Python open-source library called tiktoken (openai uses it )\n",
        "\n",
        "This library implements the BPE algorithm very efficiently basaed on source code in Rust.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "P67h2F-WJzkh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in c:\\users\\mayan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\mayan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\mayan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mayan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mayan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mayan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mayan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'DOSKEY' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tiktoken version: 0.7.0\n"
          ]
        }
      ],
      "source": [
        "print(\"tiktoken version:\",importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "Once installed , we can instantiate the BPE tokenizer from tiktoken as follows:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via an encoder method:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 6439, 8474, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "    \"of some unknown Place.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "The code above prints the following token IDs:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "We can then convert the token IDs back into text using the decode method, similar to our SimpleTokenizerV2\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof some unknown Place.\n"
          ]
        }
      ],
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "We can make  two note worthy observations based on the token IDs and decoded text above.\n",
        "\n",
        "First, the <|endoftext> token is assigned a relatively large token ID, namely , 50256.\n",
        "\n",
        "In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, hasa a total vocabulary size of 50,257\n",
        "\n",
        "with <|endoftext> being assigned the largest token ID.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "Second, the BPE tokenizer above encodes and decodes  unknown words, such as \"someunknownPlace\" correctly.\n",
        "\n",
        "The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens ?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " <div class=\"alert alert-block alert-warning\"> \n",
        "The algorithm underlying BPE break down words that aren't in its predefined vocabulary into smaller subwords units or even individual characters\n",
        "\n",
        "This enables it to handle out-of-vocabulary words.\n",
        "\n",
        "So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliear word during tokenization, it can represent it as a sequence of subword tokens or characters\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Let us take another simple example to illustrate how the BPE tokenizer deals with unknown tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[86, 32775, 1658, 67]\n",
            "werva esd\n"
          ]
        }
      ],
      "source": [
        "integers = tokenizer.encode(\"werva esd\")\n",
        "print(integers)\n",
        "\n",
        "\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CREATING INPUT-TARGET PAIRS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "To get started, we will first tokenize the whole <b> The Verdict </b> short story we worked with earlier using the BPE tokenizer\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5145\n"
          ]
        }
      ],
      "source": [
        "with open(\"text_data/the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "Executing the code above  will return 5145, the total number of tokens in the training set, applying the BPE tokenizer.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it results in a slightly more interesting text passage in the next step\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc_sample = enc_text[50:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "One of the easiest and most intuitive ways to create the input-target pairs for the next word prediction task is to create two variables x and y where x contains the input tokens and y contains the target, which are the inputs shifted by 1:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "The context size determines how many tokens are included in the input\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_size = 4 # length of the input\n",
        "\n",
        "#the context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
        "#to predict the next word in the sequence.\n",
        "#The input x is the first 4 toekns [1,2,3,4], and the target y is the next 4 tokens [2,3,4,5]\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:[290, 4920, 2241, 287]\n",
            "y:     [4920, 2241, 287, 257]\n"
          ]
        }
      ],
      "source": [
        "print(f\"x:{x}\")\n",
        "print(f\"y:     {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "Processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ],
      "source": [
        "for i in range(1,context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context,\"---->\", desired)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "Everything left of the arrow (--->) refers to the input an LLM would receive, and the token ID on the right side of the arrow represents the target token ID that the LLM is supposed to predict.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "For illustration purposes, let's repeat the previous code but convert the token IDs into text:\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context),\"---->\", tokenizer.decode([desired]))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "We've now created the input-target pairs that we can turn into use for the LLM training in upcoming chapters.\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "There's only one more task before we can turn the tokens into embeddings: implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as Pyorch tensors, which can be throught of as multidimensional arrays.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "In particular, we are intereseted in returning two tensors: an input tensor containing thet text the LLM sees and a target tensor that includes the targets for the LLm to predict\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IMPLEMENTING A DATA LOADER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and DataLoader classes.\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "\n",
        "Step 1: Tokenize the entire text\n",
        "\n",
        "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "\n",
        "Step 3: Return the total number of rows in the dataset\n",
        "\n",
        "Step 4:  Return a single row from the datset\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self,txt,tokenizer,max_length,stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "\n",
        "        # use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0,len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "\n",
        "The GPT Dataset V1 class in listing 2.5 is based on the PyTorch Dataset class.\n",
        "\n",
        "It defines how individual rows are fetched from the dataset.\n",
        "\n",
        "Each row consists of a number of token IDs (based on a max_length) assigned to an input_chunk tensor.\n",
        "\n",
        "The target_chunk tensor contains the corresponding targets.\n",
        "\n",
        "I recommend reading on to see how the data returned from this dataset looks like when we combine the dataset with additional intuition and clarity.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "The following code will use the GPT Dataset V1 to load the inputs in batches via a PyTorch DataLoader:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "\n",
        "Step 1: Initialize the tokenizer\n",
        "\n",
        "Step 2: Create dataset\n",
        "\n",
        "Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training \n",
        "\n",
        "Step 4:  The number of CPU processes to use for preprocessing\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4,max_length=256,\n",
        "                         stride=128, shuffle=True,drop_last=True, num_workers=0\n",
        "                         ):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "\n",
        "\n",
        "    # Create dataloader\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "\n",
        "Let's test the dataloader with a batch size of 1 for the LLM with a context size of 4,\n",
        "\n",
        "This will develop an intuition of how the GPTDatasetV1 class and the create_dataloader_v1 function work together:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"text_data/the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "Convert dataloader into a Python iterator to fetch the next entry via Python's build-in next() function\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version : 2.4.1+cpu\n",
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch version :\",torch.__version__)\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,batch_size=1,max_length=4,stride=1,shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "The first_batch variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs.\n",
        "\n",
        "Since the max_length is set to 4, each of the two tensors contains 4 token IDs.\n",
        "\n",
        "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least <b> 256 </b>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "To illustrate the meaning of stride=1, \n",
        "Let's fetch another batch from this dataset:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ],
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "If we compare the first with the second batch, we can see that the second batch's token IDs are shifted by one position compared to the first batch.\n",
        "\n",
        "For example, the second ID in the first batch's input is 367, which is the first ID of the second batch's input.\n",
        "\n",
        "The stride setting dictates the number of positions the inputs shift across batches, emulating a sliding window approach\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "Batch sizes of 1, such as we have sampled from the data loader so far, are useful for illustration purposes.\n",
        "\n",
        "If you have previous experience with deep learning, you may know that small batch sizes require less memory during training but lead to more noise  model updates.\n",
        "\n",
        "Just like in regular deep learning, the batch size is a trade-off and hyperparameter to experiment with when training LLMs.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "Before we move on to the two final  sections of this chapter that are focused on creating the embeddings vectors from the token IDs, let's have a brief look at how we can use the data loader to sample with a batch size greater than 1:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(raw_text,batch_size=8,max_length=4,stride=4,shuffle=False)\n",
        "\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs,target = next(data_iter)\n",
        "print(\"Inputs:\\n\",inputs)\n",
        "print(\"\\nTargets:\\n\",target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\"> \n",
        "Note that we increase the stride to 4. This is to utilize the dataset fully (we don't skip a single word) but also avoid any overlap between the batches, since more overlap could lead to increased overfitting.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CREATING TOKEN EMBEDDINGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "Let's illustrate how the token ID to embedding  vector conversion works with a hands-on example.\n",
        "\n",
        "Suppose we have the following four input tokens with IDs 2,3,5 and 1\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids = torch.tensor([2,3,5,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of only 6 words (instead of the 50,257 words in the  vocabulary), and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensins):\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch, setting the random seed to 123 for reproducibility purposes:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "The Print statement in the code prints the embeddings layer;s underlying weight matrix:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "We can see that the weight matrix of the embedding layer contains small, random values. \n",
        "These values are optimized during LLM training as part of the LLM optimization itself, as we will see in upcomin chapters. \n",
        "Moreover, we can see that the weight matrix hs six rows and three columns. There is one row for each of\n",
        "the six possilbe tokens in the vocabulary. And there is one column for each of the three embedding dimensions.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\"> \n",
        "After we instantiated the embedding layer, let's now apply it to a token ID to optain the embedding vector:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "if we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the 4th row (Python starts with a zero)\n",
        "it's the row corresponding to index 3. \n",
        "In other words, the embedding layer is essentially a look-up operation that matrix via a token ID.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(input_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\"> \n",
        "Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Previously, we focused on very small emeddings sizes in this chapter for illustration purposes.\n",
        "\n",
        "We now consider more realistic and useful embeddings sizess and encode the input toknes into a 256-dimensional vector representation.\n",
        "\n",
        "This is smaller than what the original GPT-3 model used (in GPT-3, the embedding sizse is 12,288 dimensions) but still reasonable for experimentation.\n",
        "\n",
        "Furthermore, we assume that the token IDs were created by the BPE tokenizer that we implemented earlier, which has a vocabulary size of 50,257:\n",
        "\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "Using the token_embedding_layer above, if we sample data from  the data loader, we embed each token in each batch into a 256-dimensional vector.\n",
        "\n",
        "So for batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor.\n",
        " </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Let's instantiate the data loader (Data sampling with a sliding window), first\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,batch_size=8,max_length=max_length,stride=max_length,shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "inputs,targets = next(data_iter)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\",inputs)\n",
        "print(\"\\nInputs shape:\\n\",inputs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "As we can see, the token ID tensor is 8x4-dimensinal, meaning that the data batch consists of 8 text samples with 4 tokens each.\n",
        " </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Let's now use the embedding layer to embed these token IDs into 256-dimensional vectors:\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now embedded as a 256-dimensional vector\n",
        " </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same dimensions\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "conetext_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(conetext_length,output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IMPLEMENTING A SIMPLIFIED ATTENTION MECHANISM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Consider the following input sentence, which has already been embedded into 3-dimensional vectors.\n",
        "\n",
        "We choose a small embeddings dimension for illustration purposes to ensure it fits on the page without line breaks\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0.43,0.15,0.89],   # Your  (x^1)\n",
        "        [0.55,0.87,0.66],   # journey  (x^2)\n",
        "        [0.57,0.85,0.64],   # starts  (x^3)\n",
        "        [0.22,0.58,0.33],   # with   (x^4)\n",
        "        [0.77,0.25,0.10],   # one  (x^5)\n",
        "        [0.05,0.80,0.55]    # step  (x^6)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "Each row represents a word, and each column represents an embedding dimension\n",
        " </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "The second input token servers as the query\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1]  # 2nd input token is the query\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i,x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i,query) # dot product (transpose not necessary)\n",
        "\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "In the next step, we normalize each of the attention scores that we computer previously.\n",
        " </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "The main goal behine the normalization is to obtain attention weights that sum up to  1.\n",
        "\n",
        "The normalization is a convention that is useful for interpretation and for maintaining training stability in an LLM.\n",
        "\n",
        "here's a straightforward method for achieving this normalization step:\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights : tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "\n",
        "print(\"Attention weights :\",attn_weights_2_tmp)\n",
        "print(\"Sum:\",attn_weights_2_tmp.sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "In practice, it's more common and advisable to use the softmax function for normalization.\n",
        "\n",
        "This approach is better at managing extreme values and offers more favorable gradient \n",
        "\n",
        "Below is a basic implementation of the softmax function for normalizaing the attention scores:\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights :  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "def softmax_naive(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print(\"Attention weights : \",attn_weights_2_naive)\n",
        "print(\"Sum:\",attn_weights_2_naive.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "An the output shows, the softmax function also meets the objective and normalizes the  attention weights that they sum to 1:\n",
        " </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "In addition, the softmax function ensures that the attention weights are always positive. This  makes the output interpretable as probabilities or relative  importance, where highter weights indicate greater importance.\n",
        " </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "Note that this naive softmax implementation (softmax_naive) may encounter numerical instability problems, such as overflow and underflow,when dealing with large or small input values.\n",
        "\n",
        "Threfore, in practice, it's advisable to use the PyTorch implementation of softmax.\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention weights : tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2  = torch.softmax(attn_scores_2,dim=0)\n",
        "print(\"Attention weights :\",attn_weights_2)\n",
        "print(\"Sum:\",attn_weights_2.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "in this case, we can see that it yields thesame results as our previous softmax_naive function:\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1] # 2nd input token is the query\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "\n",
        "for i,x_i in enumerate(inputs):\n",
        "    context_vec_2 += attn_weights_2[i]*x_i \n",
        "\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "from mpl_toolkits.mplot3d import Axes3D\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Now, we can extend this computation to calculate attention weights and context vectors for all inputs.\n",
        " </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "First, we add an additional for-loop to compute the dot products for all pairs of inputs.\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = torch.empty(6,6)\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "    for j, x_j in enumerate(inputs):\n",
        "        attn_scores[i,j] = torch.dot(x_i,x_j)\n",
        "\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Each element in the preceding tensor represents an attention score between each pair of inputs.\n",
        " </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "When computing the preceding attention score tensor, we used for-loops in Python.\n",
        "\n",
        "Howerver, for-loops are generally slow, and we can achieve the same results using matrix multiplication:\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores =inputs @ inputs.T\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "we now normalize each row so that the values in each row sum to 1:\n",
        "\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(attn_scores,dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "In the context of using PyTorch, the `dim` parameter in functions like `torch.softmax` specifies the dimension of the input tensor along which the function will be computed.\n",
        "\n",
        "By setting `dim=-1`, we are instructing the softmax function to apply the normalization along the last dimension of the `attn_scores` tensor.\n",
        "\n",
        "If `attn_scores` is a 2D tensor (for example, with a shape of `[rows, columns]`), `dim=-1` will normalize across the columns so that the values in each row (summing over the column dimension) sum up to 1.\n",
        "<div/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IMPLEMENTING SELF ATTENTION WITH TRAINABLE  WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0.43,0.15,0.89],   # Your  (x^1)\n",
        "        [0.55,0.87,0.66],   # journey  (x^2)\n",
        "        [0.57,0.85,0.64],   # starts  (x^3)\n",
        "        [0.22,0.58,0.33],   # with   (x^4)\n",
        "        [0.77,0.25,0.10],   # one  (x^5)\n",
        "        [0.05,0.80,0.55]    # step  (x^6)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Let's begin by defining a few variables:\n",
        "<div/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "#A The second input element\n",
        "\n",
        "#B The input embedding size, d=3\n",
        "\n",
        "#C The output embeddng size, d_out=2\n",
        "<div/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_2 = inputs[1] #A\n",
        "\n",
        "d_in = inputs.shape[1]  #B\n",
        "\n",
        "d_out = 2  #C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "Note that in GPT-like models, the input and output dimensins are usually the same,\n",
        "\n",
        "But for illustration purposes, to better follow the computation, we choose  different input (d_in=3) and output (d_out)\n",
        "<div/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<div class=\"alert alert-block alert-success\">\n",
        "Next, we initialize the three weight matrices Wq, Wk and Wv\n",
        "<div/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n"
          ]
        }
      ],
      "source": [
        "print(W_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n"
          ]
        }
      ],
      "source": [
        "print(W_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ],
      "source": [
        "print(W_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Next, we compute the query, key, and value vectors as shown earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ],
      "source": [
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key \n",
        "value_2 = x_2 @ W_value \n",
        "print(query_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### As we can see based on the output for the query, this results in a 2-dimensional vector.\n",
        "\n",
        "#### This is because: we set the number of columns of the corresponding weight matrix, via d_out, to 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we can obtain all keys and values via matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n",
            "queries.shape: torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "queries = inputs @ W_query\n",
        "\n",
        "print(\"Keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)\n",
        "print(\"queries.shape:\", queries.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "First, let's compute the attention score 22\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ],
      "source": [
        "keys_2 = keys[1] #A\n",
        "attn_scores_22 = query_2.dot(keys_2)\n",
        "print(attn_scores_22)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Again, we can generalize this computation to all  attention scores via matrix multiplication:\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ],
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
            "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
            "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
            "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
            "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
            "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = queries @ keys.T # omega\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2/ d_k**0.5, dim=-1)\n",
        "\n",
        "print(attn_weights_2)\n",
        "print(d_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## WHY DIVIDE BY SQRT (DIMENSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "Reason 1: For stability in learning\n",
        "\n",
        "Reason 2: The softmax function function is sensitive to the magnitutes , where higestest value will get all the marks\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2897, 0.8043],\n",
            "        [0.3069, 0.8188],\n",
            "        [0.3063, 0.8173],\n",
            "        [0.2972, 0.7936],\n",
            "        [0.2848, 0.7650],\n",
            "        [0.3043, 0.8105]])\n"
          ]
        }
      ],
      "source": [
        "context_vec_2 = attn_weights @ values\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "So far, we only computed a single context vector, z(2).\n",
        "\n",
        "In the next section, we will generalize the code to compute all context vectors in the input sequence, z(1) to z (T)\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "In the previous sections, we have gone through a lot of steps to compute the self-attention outputs.\n",
        "\n",
        "This was mainly done for illustraton purposes so we could go through one step at a time.\n",
        "\n",
        "In practice, with the LLM implementation in the next chapter in mind, it is helpful to organize this code into a Python\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self,d_in,d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in,d_out))\n",
        "        self.W_key = nn.Parameter(torch.rand(d_in,d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in,d_out))\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "\n",
        "        attn_scores = queries @ keys.T # omega\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores/keys.shape[-1]**0.5, dim=-1\n",
        "            )\n",
        "        \n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a fundamental building block of PyTorch mod functionalities for model layer creation and management.\n",
        "\n",
        "The init method initializes trainable weight matrices (W_query, W_key, and W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out.\n",
        "\n",
        "In the forward pass, using the forward method, we compute the attention scores (attn_scores) by multiplying queries and keys.\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in,d_out)\n",
        "print(sa_v1(inputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "    def __init__(self,d_in,d_out,qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5,dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values \n",
        "        return context_vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HIDING FUTURE WORDS WITH CAUSAL ATTENTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Let's work with the attention scores and weights form the previous section to code the causal attention mechanism.\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "queries = sa_v2.W_query(inputs)  #A\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T \n",
        "attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "We can now use PyTorch's tril function to create a mask where the values above the diagonal are zero:\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple  = torch.tril(torch.ones(context_length,context_length))\n",
        "print(mask_simple)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Now, we can multiply this mask with the attention weights to zero out the values  above the diagonal:\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "masked_simple = attn_weights*mask_simple\n",
        "print(masked_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "As we can see, the elements above the diagonal are successfully zeroed out\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "The third step is to renormalize the attention weights to sum up to 1 again in each row.\n",
        "We can achieve this by dividing each element in each row by the sum in each row:\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "row_sums = masked_simple.sum(dim=1,keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "The result is an attention weights matrix where the attention weights above the diagonal are zeroed out \n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mask = torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(),-torch.inf)\n",
        "print(masked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now, all we need to do is apply the softmax function to these masked results, and we are done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5,dim=1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        " we can see based on the output, the values in each row sum to 1, and no further normalization is necessary.\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MASKING ADDITIONAL ATTENTION WEIGHTS WITH DROPOUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "In the following code example, we use a dropout rate of 50%, which means masking out half of the attention weights.\n",
        "\n",
        "When we train the GPT model , we will use a lower dropout rate, such as 0.1 or 0.2\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) #A\n",
        "example = torch.ones(6,6) #B\n",
        "print(dropout(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero.\n",
        "To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 = 2.\n",
        "\n",
        "This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases.\n",
        "\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we need to ensure if code can handle batches consisting of more than one input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "batch = torch.stack((inputs,inputs),dim=0)\n",
        "print(batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self,d_in,d_out,context_length,dropout,qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query =  nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "        self.W_key =  nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "        self.W_value =  nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1)) \n",
        "\n",
        "    def forward(self,x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1,2) # Changed transpose\n",
        "        attn_scores.masked_fill_(  # New, _ opsare in-place\n",
        "            self.mask.bool() [:num_tokens,:num_tokens],-torch.inf)\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim = -1\n",
        "        )\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out,context_length,0.0)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llmstrach",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
