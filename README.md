 # Large Language Model (LLM) from Scratch


![llm_from_sratch](https://github.com/user-attachments/assets/879e097c-b9d1-4284-b173-405ca8457848)
This repository provides a step-by-step implementation of a **Large Language Model (LLM) from scratch**, covering **data preparation, model building, pretraining, and fine-tuning**. The project includes notebooks and scripts to help understand the complete process of training an LLM.

# MathGPT
![MathGPT](https://github.com/user-attachments/assets/199ebf74-8f57-48d5-9896-0e11ceb2013a)


## ðŸ“Œ Project Overview

The project follows **three main stages** in developing an LLM:

1. **Building an LLM**:  
   - Data preparation & sampling  
   - Attention mechanisms  
   - Defining the LLM architecture  

2. **Pretraining**:  
   - Training the foundation model on large-scale text data  
   - Learning word embeddings and transformer layers  

3. **Fine-tuning**:  
   - Using domain-specific datasets  
   - Adapting the model for specialized tasks like classification or chat assistants  


ðŸ“Š Results
Accuracy & Loss Visualizations:

accuracy-plot

![accuracy-plot_page-0001](https://github.com/user-attachments/assets/25b72e25-4bba-4c6d-97a6-1ecdda0440fd)

loss-plot.pdf 

![loss-plot_page-0001](https://github.com/user-attachments/assets/43a400d8-442f-48e5-a49f-80d23f1d2755)




Fine-tuned Model Applications:

- Classifier: Trained using labeled data for spam detection.
- Assistant: Uses instruction tuning for conversational AI.

To get started with this project, clone the repository and set up your environment:

```
git clone https://github.com/Mayankpratapsingh022/LLM_from_Scratch/tree/master/GPT2-124M_from_Scratch
cd LLLM_from_Scratch
pip install -r requirements.txt
```
After setting up, explore the Jupyter notebooks and scripts to train and fine-tune your own LLM!
